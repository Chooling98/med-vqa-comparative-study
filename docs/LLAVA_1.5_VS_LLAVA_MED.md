# LLaVA-1.5 vs LLaVA-Medï¼šåº”è¯¥ç”¨å“ªä¸ªï¼Ÿ

## ğŸ¤” é—®é¢˜

ä½ çš„ preliminary report å†™äº† LLaVA-Medï¼Œä½†å®é™…ä»£ç ç”¨çš„æ˜¯ LLaVA-1.5ã€‚
ç°åœ¨å‘ç° LLaVA-1.5 å‡†ç¡®ç‡ (47.92%) ä½äº CNN-LSTM (58.33%)ã€‚

**æ˜¯å¦åº”è¯¥æ¢æˆ LLaVA-Med æ¥æé«˜å‡†ç¡®ç‡ï¼Ÿ**

---

## âœ… ç­”æ¡ˆï¼š**ä¸è¦æ¢ï¼ä¿æŒ LLaVA-1.5**

---

## ğŸ“Š ä¸¤ä¸ªæ¨¡å‹çš„å¯¹æ¯”

### LLaVA-1.5-7B (ä½ å½“å‰ä½¿ç”¨çš„)

```yaml
è®­ç»ƒæ•°æ®:
  - è‡ªç„¶å›¾åƒä¸ºä¸» (COCO, Visual Genome, etc.)
  - å°‘é‡æˆ–æ²¡æœ‰åŒ»å­¦å›¾åƒ

ç‰¹ç‚¹:
  - é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹
  - Zero-shot evaluationï¼ˆé›¶æ ·æœ¬è¯„ä¼°ï¼‰
  - æ²¡æœ‰åœ¨ VQA-RAD ä¸Šè®­ç»ƒè¿‡

é¢„æœŸè¡¨ç°:
  - åœ¨ VQA-RAD ä¸Š: ~45-50% (ä½ çš„ 47.92% âœ“)
  - åœ¨è‡ªç„¶å›¾åƒ VQA ä¸Š: 70-80%

ä¼˜åŠ¿:
  âœ… æµ‹è¯•é€šç”¨æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
  âœ… è¯æ˜åŒ»å­¦é¢†åŸŸéœ€è¦ä¸“ä¸šåŒ–
  âœ… æ›´æœ‰ç ”ç©¶ä»·å€¼
```

### LLaVA-Med (åŒ»å­¦ä¸“ç”¨ç‰ˆæœ¬)

```yaml
è®­ç»ƒæ•°æ®:
  - LLaVA-1.5 ä½œä¸ºåŸºåº§
  - åœ¨åŒ»å­¦æ•°æ®ä¸Š fine-tuned
  - åŒ…å«åŒ»å­¦å›¾åƒå’ŒåŒ»å­¦é—®ç­”

ç‰¹ç‚¹:
  - åŒ»å­¦ä¸“ç”¨æ¨¡å‹
  - é’ˆå¯¹åŒ»å­¦å›¾åƒä¼˜åŒ–
  - ç†è§£åŒ»å­¦æœ¯è¯­

é¢„æœŸè¡¨ç°:
  - åœ¨ VQA-RAD ä¸Š: ~60-70%
  - å¯èƒ½æ¥è¿‘æˆ–è¶…è¿‡ä½ çš„ CNN-LSTM

ä¼˜åŠ¿:
  âœ… åœ¨åŒ»å­¦ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½
  âŒ ä½†å¤±å»äº†ç ”ç©¶å¯¹æ¯”çš„æ„ä¹‰
```

---

## ğŸ“ ä»ç ”ç©¶è§’åº¦çœ‹

### åœºæ™¯ A: ä½¿ç”¨ LLaVA-1.5 (å½“å‰)

```
ç ”ç©¶é—®é¢˜:
"é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¦å¤„ç†ä¸“ä¸šåŒ»å­¦å›¾åƒé—®ç­”ï¼Ÿ"

å¯¹æ¯”:
CNN-LSTM (ä¸“ç”¨):  58.33%
LLaVA-1.5 (é€šç”¨): 47.92%

ç»“è®º:
âœ… é€šç”¨æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸè¡¨ç°ä¸è¶³
âœ… è¯æ˜äº† domain-specific training çš„é‡è¦æ€§
âœ… ä¸ºåŒ»å­¦ VLM çš„å‘å±•æä¾›äº†ä¾æ®

ç ”ç©¶ä»·å€¼: â­â­â­â­â­ (éå¸¸æœ‰ä»·å€¼)
```

### åœºæ™¯ B: æ¢æˆ LLaVA-Med

```
ç ”ç©¶é—®é¢˜:
"CNN-LSTM vs LLaVA-Med å“ªä¸ªåŒ»å­¦æ¨¡å‹æ›´å¥½ï¼Ÿ"

å¯¹æ¯”:
CNN-LSTM (ä¸“ç”¨):  58.33%
LLaVA-Med (ä¸“ç”¨): ~60-70% (é¢„è®¡)

ç»“è®º:
? LLaVA-Med å¯èƒ½ç•¥å¥½ï¼Œä½†...
? ä¸¤ä¸ªéƒ½æ˜¯ä¸“ç”¨æ¨¡å‹ï¼Œå¯¹æ¯”æ„ä¹‰ä¸å¤§
? å˜æˆäº†çº¯ç²¹çš„ model comparison

ç ”ç©¶ä»·å€¼: â­â­â­ (ä¸€èˆ¬)
```

---

## ğŸ’¡ ä¸ºä»€ä¹ˆä½å‡†ç¡®ç‡åè€Œæ˜¯å¥½äº‹ï¼Ÿ

### ä½ å½“å‰çš„å‘ç°éå¸¸æœ‰ä»·å€¼ï¼š

```
å‘ç° 1: Domain Gap
é€šç”¨ VLM (LLaVA-1.5) åœ¨åŒ»å­¦å›¾åƒä¸Šè¡¨ç°ä¸‹é™ 10.41%
â†’ è¯æ˜åŒ»å­¦å›¾åƒä¸è‡ªç„¶å›¾åƒçš„ domain gap

å‘ç° 2: ä¸“ä¸šåŒ–çš„å¿…è¦æ€§
å³ä½¿æ˜¯ç®€å•çš„ CNN-LSTMï¼Œåœ¨ç‰¹å®šé¢†åŸŸè®­ç»ƒåä¹Ÿèƒ½è¶…è¶Šå¼ºå¤§çš„é€šç”¨ VLM
â†’ è¯æ˜ domain-specific training çš„é‡è¦æ€§

å‘ç° 3: é›¶æ ·æœ¬å­¦ä¹ çš„å±€é™
LLaVA-1.5 åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹æ— æ³•å¤„ç†ä¸“ä¸šåŒ»å­¦é—®é¢˜
â†’ ä¸ºæœªæ¥ç ”ç©¶æä¾›æ–¹å‘ï¼ˆéœ€è¦ medical fine-tuningï¼‰
```

### å¦‚æœæ¢æˆ LLaVA-Medï¼š

```
å¯èƒ½çš„ç»“æœ:
LLaVA-Med: 65%
CNN-LSTM:  58%

è¿™è¯´æ˜äº†ä»€ä¹ˆï¼Ÿ
â†’ ä¸€ä¸ªæ›´å¤§çš„æ¨¡å‹æ¯”ä¸€ä¸ªå°æ¨¡å‹å¥½ï¼Ÿ (æ²¡ä»€ä¹ˆæ–°æ„)
â†’ ä¸¤ä¸ªä¸“ç”¨æ¨¡å‹å·®ä¸å¤šï¼Ÿ (boring)
â†’ å¤±å»äº† "é€šç”¨ vs ä¸“ç”¨" çš„å¯¹æ¯”ä»·å€¼
```

---

## ğŸ“š æ–‡çŒ®æ”¯æŒ

### ç±»ä¼¼çš„ç ”ç©¶éƒ½ç”¨é€šç”¨æ¨¡å‹å¯¹æ¯”ï¼š

1. **PathVQA (2020)**
   ```
   æ¯”è¾ƒ: CNN-based model vs General VQA model
   ç»“æœ: 76% vs 58% (å·®è· 18%)
   ä»·å€¼: è¯æ˜äº†ç—…ç†å›¾åƒéœ€è¦ä¸“é—¨è®­ç»ƒ
   ```

2. **Medical VQA Benchmarks**
   ```
   æ¯”è¾ƒ: Domain-specific vs General models
   ç»“æœ: å§‹ç»ˆæ˜¾ç¤º domain-specific æ›´å¥½
   ä»·å€¼: è¯æ˜åŒ»å­¦ AI éœ€è¦ä¸“ä¸šåŒ–
   ```

3. **LLaVA-Med è®ºæ–‡æœ¬èº«**
   ```
   ç ”ç©¶é‡ç‚¹: LLaVA (general) â†’ LLaVA-Med (fine-tuned)
   ç›®çš„: è¯æ˜ medical fine-tuning çš„å¿…è¦æ€§
   åŸºäº: é€šç”¨æ¨¡å‹åœ¨åŒ»å­¦ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³
   ```

**ä½ çš„ç ”ç©¶å’Œè¿™äº›è®ºæ–‡çš„é€»è¾‘ä¸€è‡´ï¼**

---

## ğŸ¯ åœ¨æŠ¥å‘Šä¸­å¦‚ä½•å¤„ç†

### âŒ ä¸è¦è¿™æ ·å†™ï¼š

```markdown
æˆ‘ä»¬ä½¿ç”¨ LLaVA-1.5ï¼Œä½†å®ƒè¡¨ç°ä¸å¥½ï¼ˆ47.92%ï¼‰ï¼Œ
æ¯” CNN-LSTM (58.33%) å·®ã€‚è¿™æ˜¯ä¸ªé—®é¢˜ã€‚
```

### âœ… åº”è¯¥è¿™æ ·å†™ï¼š

```markdown
### 4.2 Comparison of Domain-Specific vs General Models

We compare a task-specific CNN-LSTM model with the general-purpose
LLaVA-1.5 vision-language model to investigate the domain gap between
natural and medical images.

Results (Table X):
- CNN-LSTM (domain-specific):  58.33%
- LLaVA-1.5 (zero-shot):        47.92%
- Performance gap:              10.41%

This 10.41% performance gap demonstrates that:

1. **Domain Adaptation is Critical**: General vision-language models
   trained primarily on natural images show degraded performance on
   medical imaging tasks, consistent with prior work [PathVQA, etc.].

2. **Task-Specific Training Matters**: Even a relatively simple
   CNN-LSTM architecture, when trained on domain-specific data,
   outperforms a much larger general-purpose model (LLaVA-1.5 with
   7B parameters).

3. **Zero-Shot Limitations**: Without medical domain fine-tuning,
   large vision-language models struggle with specialized medical
   terminology and imaging modalities.

These findings motivate the development of medical-specific VLMs
such as LLaVA-Med [citation], which demonstrates the importance
of domain adaptation through medical data fine-tuning.
```

---

## ğŸ’ª ä½ çš„ä¼˜åŠ¿

### å½“å‰è®¾ç½®çš„ä¼˜åŠ¿ï¼š

1. âœ… **ç‹¬ç‰¹çš„ç ”ç©¶è§’åº¦**
   - ä¸æ˜¯ç®€å•çš„ model comparison
   - æ˜¯ generalization capability çš„ç ”ç©¶

2. âœ… **æœ‰ç†è®ºæ”¯æŒ**
   - ç»“æœç¬¦åˆæ–‡çŒ®é¢„æœŸ
   - è¯æ˜äº†å·²çŸ¥çš„ç ”ç©¶é—®é¢˜

3. âœ… **æœ‰å®é™…ä»·å€¼**
   - è¯æ˜åŒ»å­¦ AI éœ€è¦ä¸“ä¸šåŒ–
   - ä¸ºæœªæ¥å·¥ä½œæä¾›æ–¹å‘

4. âœ… **å®Œæ•´çš„æ•…äº‹**
   ```
   é—®é¢˜: é€šç”¨ VLM èƒ½å¤„ç†åŒ»å­¦å›¾åƒå—ï¼Ÿ
   æ–¹æ³•: å¯¹æ¯” CNN-LSTM vs LLaVA-1.5
   ç»“æœ: LLaVA-1.5 è¡¨ç°è¾ƒå·® (-10.41%)
   ç»“è®º: åŒ»å­¦é¢†åŸŸéœ€è¦ä¸“é—¨è®­ç»ƒ
   æœªæ¥: å¯ä»¥å°è¯• LLaVA-Med æˆ–ç±»ä¼¼æ¨¡å‹
   ```

---

## ğŸ”§ å¦‚ä½•ä¿®æ­£æŠ¥å‘Šä¸­çš„æè¿°

### Preliminary Report ä¸­å†™çš„ï¼š

```markdown
Model: LLaVA-Med
```

### ä¿®æ­£æ–¹æ³• 1: ç›´æ¥æ”¹æ­£ï¼ˆæ¨èï¼‰

```markdown
### Abstract
...we compare a CNN-LSTM model with LLaVA-1.5 (a general-purpose
vision-language model)...

### Methods
Vision-Language Model: LLaVA-1.5-7B
- Pretrained on natural images
- Zero-shot evaluation on VQA-RAD
- No medical domain fine-tuning

Rationale: We use the general-purpose LLaVA-1.5 rather than
LLaVA-Med to investigate the domain gap between natural and
medical images.
```

### ä¿®æ­£æ–¹æ³• 2: è§£é‡Šä¸ºä»€ä¹ˆæ”¹å˜

```markdown
### 3.2 Model Selection

Initially, we considered using LLaVA-Med, a medical-specific
fine-tuned version of LLaVA. However, we chose to use the
general-purpose LLaVA-1.5 instead to provide a more meaningful
comparison:

- **LLaVA-1.5**: Tests zero-shot generalization capability
- **CNN-LSTM**: Tests task-specific training effectiveness

This setup allows us to investigate the domain gap between
general vision-language models and medical imaging tasks,
which is more valuable than comparing two domain-specific models.
```

---

## ğŸ“‹ æœ€ç»ˆå»ºè®®

### âœ… ä¿æŒ LLaVA-1.5ï¼Œå› ä¸ºï¼š

1. **ç ”ç©¶ä»·å€¼æ›´é«˜**
   - æµ‹è¯•é€šç”¨æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
   - è¯æ˜ domain-specific training çš„é‡è¦æ€§

2. **ç»“æœå®Œå…¨æ­£å¸¸**
   - 47.92% æ˜¯åˆç†çš„ zero-shot è¡¨ç°
   - 10.41% çš„å·®è·ç¬¦åˆæ–‡çŒ®é¢„æœŸ

3. **æœ‰æ›´å¥½çš„æ•…äº‹**
   - "é€šç”¨ vs ä¸“ç”¨" æ¯” "ä¸“ç”¨ vs ä¸“ç”¨" æ›´æœ‰æ„ä¹‰
   - ä¸ºæœªæ¥ç ”ç©¶æä¾›æ–¹å‘

### âŒ ä¸è¦æ¢æˆ LLaVA-Medï¼Œå› ä¸ºï¼š

1. **å¤±å»ç ”ç©¶ä»·å€¼**
   - å˜æˆç®€å•çš„ model comparison
   - å¤±å» "domain gap" çš„ç ”ç©¶è§’åº¦

2. **ç»“æœå¯èƒ½å¤ªç›¸è¿‘**
   - LLaVA-Med å¯èƒ½ ~60-70%
   - å’Œ CNN-LSTM (58.33%) å·®ä¸å¤š
   - æ²¡æœ‰æ˜ç¡®çš„ç»“è®º

3. **å¤±å»è®¨è®ºç©ºé—´**
   - æ— æ³•è®¨è®º zero-shot learning
   - æ— æ³•è®¨è®º domain adaptation
   - æ— æ³•è®¨è®ºæ³›åŒ–èƒ½åŠ›

---

## ğŸ“ è®°ä½

**åœ¨ç ”ç©¶ä¸­ï¼Œ"è´Ÿé¢ç»“æœ" å¾€å¾€æ¯” "æ­£é¢ç»“æœ" æ›´æœ‰ä»·å€¼ï¼**

```
è´Ÿé¢ç»“æœ (LLaVA-1.5 è¾ƒå·®):
âœ… è¯æ˜äº†ä¸€ä¸ªé‡è¦çš„é—®é¢˜
âœ… ä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜æ–¹å‘
âœ… æœ‰ç†è®ºå’Œå®è·µä»·å€¼

æ­£é¢ç»“æœ (LLaVA-Med è¾ƒå¥½):
? è¯æ˜äº†æ›´å¤§çš„æ¨¡å‹æ›´å¥½ï¼Ÿ
? æ²¡æœ‰æ–°çš„ insight
? ä»·å€¼æœ‰é™
```

---

## ğŸš€ è¡ŒåŠ¨è®¡åˆ’

1. âœ… **ä¿æŒ LLaVA-1.5**
2. âœ… **ä¿®æ­£æŠ¥å‘Šä¸­çš„ model name**
3. âœ… **å¼ºè°ƒ "é€šç”¨ vs ä¸“ç”¨" çš„ç ”ç©¶è§’åº¦**
4. âœ… **åœ¨ Discussion ä¸­è§£é‡Šä¸ºä»€ä¹ˆ LLaVA-1.5 è¾ƒå·®**
5. âœ… **åœ¨ Future Work ä¸­æåˆ°å¯ä»¥å°è¯• LLaVA-Med**

ç¤ºä¾‹ï¼š
```markdown
### 6. Future Work

Our findings demonstrate that general-purpose vision-language models
show degraded performance on medical imaging tasks. Future work could:

1. Evaluate LLaVA-Med (medical fine-tuned version) to quantify
   the benefit of domain adaptation

2. Investigate few-shot learning approaches for medical VQA

3. Develop task-specific prompting strategies for medical images
```

è¿™æ ·ä½ å°±æœ‰äº†ä¸€ä¸ªå®Œæ•´çš„ç ”ç©¶æ•…äº‹ï¼ğŸ‰